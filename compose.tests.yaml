services:
  prompt-server:
    build:
      context: .
      dockerfile: sre_agent/servers/prompt_server/Dockerfile
    environment:
      - GITHUB_ORGANISATION=fuzzylabs
      - GITHUB_REPO_NAME=microservices-demo
      - PROJECT_ROOT=src

  llm-server:
    build:
      context: .
      dockerfile: sre_agent/llm/Dockerfile
    environment:
      - PROVIDER=mock
      - ANTHROPIC_API_KEY=null
      - GEMINI_API_KEY=null

  llama-firewall:
    build:
      context: .
      dockerfile: sre_agent/firewall/Dockerfile
    volumes:
      - source: ~/.cache/huggingface
        target: /root/.cache/huggingface
        type: bind
        bind:
          create_host_path: true
    environment:
      - HF_TOKEN=${HF_TOKEN}
    ports:
      - "8000:8000"

  orchestrator:
    build:
      context: .
      dockerfile: sre_agent/client/Dockerfile
    ports:
      - "8003:80"

    depends_on:
      prompt-server:
        condition: service_healthy
      llm-server:
        condition: service_healthy
      llama-firewall:
        condition: service_healthy

    environment:
      - DEV_BEARER_TOKEN=password
      - QUERY_TIMEOUT=300
      - SLACK_SIGNING_SECRET=null
      - TOOLS=["list_pods", "get_logs", "get_file_contents", "slack_post_message"]
      - SLACK_CHANNEL_ID=null
      - SERVICES=["cartservice", "adservice", "emailservice"]
